import os
import random
import pandas as pd  # ç”¨æ¥å¤„ç†è¡¨æ ¼æ•°æ®çš„å·¥å…·
import numpy as np   # ç”¨æ¥åšæ•°å­¦è®¡ç®—çš„å·¥å…·
from collections import defaultdict  # ç”¨æ¥å­˜å‚¨ç»“æœçš„ç‰¹æ®Šå­—å…¸

import torch  # æ·±åº¦å­¦ä¹ ç”¨çš„æ¡†æ¶
# ä¸‹é¢ä¸¤ä¸ªæ˜¯å¤„ç†BERTæ¨¡å‹çš„å·¥å…·ï¼ŒBERTæ˜¯ä¸€ä¸ªèƒ½ç†è§£æ–‡å­—çš„AIæ¨¡å‹
from transformers import BertTokenizer, BertModel
from sklearn.linear_model import LogisticRegression  # é€»è¾‘å›å½’åˆ†ç±»å™¨ï¼ˆä¸€ç§å¼ºå¤§çš„åˆ†ç±»ç®—æ³•ï¼‰
# ä¸‹é¢æ˜¯ä¸€äº›è¯„ä»·æ¨¡å‹å¥½åçš„æŒ‡æ ‡
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import KFold, GridSearchCV  # ç”¨æ¥åšäº¤å‰éªŒè¯å’Œç½‘æ ¼æœç´¢çš„å·¥å…·
from sklearn.decomposition import PCA  # ä¸»æˆåˆ†åˆ†æï¼Œç”¨äºé™ç»´å’Œç‰¹å¾é€‰æ‹©
from sklearn.preprocessing import StandardScaler  # æ ‡å‡†åŒ–å·¥å…·ï¼ŒPCAä¹‹å‰éœ€è¦æ ‡å‡†åŒ–

# basically the same as the import in BERT_RF.py
# except for the support_vector_machine 

# note that logistic_regression is excellent tool for binary classification: 
# goal: hybrid machine learning models
# BERT: Bidirectional Encoder Representation with Transformers
# LR: Logistic Regression
# predict the suicidal risk of patients

# å›ºå®šéšæœºæ•°ï¼Œè®©æ¯æ¬¡è¿è¡Œç»“æœéƒ½ä¸€æ ·ï¼Œæ–¹ä¾¿æ¯”è¾ƒ
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# è®¾å¤‡é…ç½®å’Œæ£€æµ‹ (æ”¯æŒApple Silicon MPS, CUDA, CPU)
def get_optimal_device():
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡"""
    if torch.cuda.is_available():
        return torch.device('cuda'), 'NVIDIA GPU'
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        return torch.device('mps'), 'Apple Silicon GPU (MPS)'
    else:
        return torch.device('cpu'), 'CPU'

DEVICE, device_name = get_optimal_device()
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {DEVICE} ({device_name})")

if DEVICE.type == 'cuda':
    print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)}")
    print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    torch.cuda.manual_seed(SEED)
elif DEVICE.type == 'mps':
    print(f"   Apple Siliconä¼˜åŒ–: âœ…")
    print(f"   ç»Ÿä¸€å†…å­˜æ¶æ„: âœ…") 
    print(f"   å»ºè®®å†…å­˜: 16GB+ æ¨è")
    torch.mps.manual_seed(SEED)
else:
    print("   ä½¿ç”¨CPUæ¨¡å¼ - å¯ç”¨ä¼˜åŒ–:")
    print("   - å¤šçº¿ç¨‹å¤„ç†: âœ…")
    print("   - æ‰¹é‡å¤„ç†: âœ…")
    print("   - PCAé™ç»´: âœ…")

# æƒ³è±¡ä½ æœ‰ä¸€å‰¯768è‰²çš„å½©è™¹ç”»ï¼ˆBERTçš„768ç»´ç‰¹å¾ï¼‰ï¼Œä½†ä½ è¦æŠŠå®ƒè£…è¿›ä¸€ä¸ªåªæœ‰256è‰²çš„è°ƒè‰²ç›˜é‡Œï¼ˆPCAé™ç»´åˆ°256ç»´ï¼‰ã€‚
# ä½ ä¼šæŒ‘å‡ºæœ€èƒ½ä»£è¡¨æ•´ä½“è‰²å½©çš„256ç§é¢œè‰²ï¼Œè¿™æ ·ç”»å‡ºæ¥çš„ç”»ä¾ç„¶ç¾ä¸½ã€ä¿¡æ¯ä¸°å¯Œï¼Œä½†æ›´è½»ä¾¿ã€æ˜“äºæºå¸¦å’Œå¤„ç†ã€‚
PCA_COMPONENTS = 256  # ç”¨PCAæŠŠ768ç»´BERTå‘é‡â€œæµ“ç¼©â€ä¸º256ç»´ï¼Œä¿ç•™ä¸»è¦ä¿¡æ¯ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
print(f"ğŸ”§ PCAé™ç»´é…ç½®: {768} â†’ {PCA_COMPONENTS} ç»´")



# 1. è¯»å–æ•°æ® (we are using the same data as BERT_RF.py)
# è¯»å–æ–‡æœ¬æ•°æ®æ–‡ä»¶ï¼ˆé‡Œé¢æœ‰ç”¨æˆ·IDå’Œå¯¹åº”çš„æ–‡å­—å†…å®¹ï¼‰
text_data = pd.read_csv("BERT_RF.py/features_linguistic_cleaned.csv")
# è¯»å–æ ‡ç­¾æ•°æ®æ–‡ä»¶ï¼ˆé‡Œé¢æœ‰ç”¨æˆ·IDå’Œå¯¹åº”çš„åˆ†ç±»æ ‡ç­¾ï¼Œæ¯”å¦‚å¥½/åã€æ˜¯/å¦ç­‰ï¼‰
Label_data = pd.read_excel("BERT_RF.py/EMA_Dataset.xlsx")

# æŠŠä¸Šé¢ä¸¤ä¸ªè¡¨é€šè¿‡"ID"åˆå¹¶èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†
data = pd.merge(text_data, Label_data, on="ID")
# æå–æ‰€æœ‰ä¸é‡å¤çš„ç”¨æˆ·IDï¼Œåé¢ä¼šæŒ‰ç”¨æˆ·æ¥åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
# è¿™æ ·åšæ˜¯ä¸ºäº†é¿å…åŒä¸€ä¸ªäººçš„æ•°æ®æ—¢ç”¨æ¥è®­ç»ƒåˆç”¨æ¥æµ‹è¯•ï¼Œä¿è¯å…¬å¹³
users = list(set(data["ID"]))

# æ‰“å°ä¸€ä¸‹æ•°æ®åŸºæœ¬æƒ…å†µ
print(f"æ€»ç”¨æˆ·æ•°: {len(users)}ï¼Œæ€»æ ·æœ¬æ•°: {len(data)}")


# 2. ç”¨BERTæ¨¡å‹æŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—
#    ï¼ˆBERTåªç”¨æ¥æå–ç‰¹å¾ï¼Œä¸é‡æ–°è®­ç»ƒï¼‰

# ç”¨çš„æ˜¯ç²¤è¯­çš„BERTæ¨¡å‹
BERT_MODEL = "indiejoseph/bert-base-cantonese"
# æ–‡å­—æœ€é•¿ä¿ç•™128ä¸ªè¯ï¼Œå¤ªé•¿çš„ä¼šæˆªæ–­ï¼Œå¤ªçŸ­çš„ä¼šè¡¥å…¨
MAX_LEN = 128

# åŠ è½½åˆ†è¯å™¨ï¼ˆæŠŠå¥å­æ‹†æˆä¸€ä¸ªä¸ªè¯æˆ–å­—çš„å·¥å…·ï¼‰
print("ğŸ“¥ æ­£åœ¨åŠ è½½BERTåˆ†è¯å™¨...")
tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)

# åŠ è½½BERTæ¨¡å‹ï¼ˆå·²ç»è®­ç»ƒå¥½çš„ï¼Œä¼šæŠŠæ–‡å­—å˜æˆæœ‰æ„ä¹‰çš„æ•°å­—ï¼‰
print("ğŸ“¥ æ­£åœ¨åŠ è½½BERTæ¨¡å‹...")
bert_model = BertModel.from_pretrained(BERT_MODEL)

# å°†BERTæ¨¡å‹ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
bert_model = bert_model.to(DEVICE)
# æŠŠBERTè®¾ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆä¸ç”¨æ¥è®­ç»ƒï¼Œåªæ˜¯ç”¨æ¥è½¬æ¢æ–‡å­—ï¼‰
bert_model.eval()
print(f"âœ… BERTæ¨¡å‹å·²åŠ è½½åˆ° {DEVICE}")

# æ ¹æ®è®¾å¤‡ç±»å‹å¯ç”¨ç›¸åº”ä¼˜åŒ–
if DEVICE.type == 'cuda':
    bert_model = bert_model.half()  # ä½¿ç”¨FP16ç²¾åº¦ï¼ŒèŠ‚çœæ˜¾å­˜å’ŒåŠ é€Ÿ
    print("âš¡ å¯ç”¨CUDA FP16åŠç²¾åº¦æ¨ç†åŠ é€Ÿ")
elif DEVICE.type == 'mps':
    # MPSç›®å‰ä¸å®Œå…¨æ”¯æŒFP16ï¼Œä½¿ç”¨FP32ä½†å¯ç”¨å…¶ä»–ä¼˜åŒ–
    print("âš¡ å¯ç”¨Apple Silicon MPSåŠ é€Ÿ")
    print("   æ³¨æ„: ä½¿ç”¨FP32ç²¾åº¦ç¡®ä¿å…¼å®¹æ€§")
else:
    # CPUæ¨¡å¼ä¼˜åŒ–
    torch.set_num_threads(os.cpu_count())  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    print(f"âš¡ å¯ç”¨CPUå¤šçº¿ç¨‹ä¼˜åŒ– (çº¿ç¨‹æ•°: {os.cpu_count()})")

def get_bert_embedding(text):
    """
    æŠŠä¸€å¥è¯è½¬æ¢æˆæ•°å­—å‘é‡ï¼ˆæ–¹ä¾¿è®¡ç®—æœºå¤„ç†ï¼‰- GPUåŠ é€Ÿç‰ˆæœ¬
    ç®€å•è¯´å°±æ˜¯ï¼šæ–‡å­— -> BERTæ¨¡å‹ -> ä¸€ä¸²æ•°å­—
    """
    # å¦‚æœè¾“å…¥çš„ä¸æ˜¯æ–‡å­—ï¼Œæˆ–è€…æ˜¯ç©ºå†…å®¹ï¼Œå°±è¿”å›ä¸€ä¸²0
    if not isinstance(text, str) or len(text.strip()) == 0:
        return np.zeros(bert_model.config.hidden_size)

    # æŠŠæ–‡å­—å¤„ç†æˆBERTèƒ½ç†è§£çš„æ ¼å¼
    # åŒ…æ‹¬ï¼šåˆ†è¯ã€è¡¥å…¨åˆ°128é•¿åº¦ã€è½¬æˆæ•°å­—
    inputs = tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=MAX_LEN)
    
    # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    
    # ä¸è®¡ç®—æ¢¯åº¦ï¼ˆèŠ‚çœå†…å­˜ï¼ŒåŠ å¿«é€Ÿåº¦ï¼Œå› ä¸ºæˆ‘ä»¬ä¸è®­ç»ƒæ¨¡å‹ï¼‰
    with torch.no_grad():
        # æŠŠå¤„ç†å¥½çš„æ–‡å­—è¾“å…¥BERTï¼Œå¾—åˆ°è¾“å‡º
        if DEVICE.type == 'cuda':
            with torch.cuda.amp.autocast():
                outputs = bert_model(**inputs)
        elif DEVICE.type == 'mps':
            # Apple Silicon MPSæ¨¡å¼ï¼šæ ‡å‡†æ¨ç†
            outputs = bert_model(**inputs)
        else:
            # CPUæ¨¡å¼ï¼šæ­£å¸¸æ¨ç†
            outputs = bert_model(**inputs)
    
    # ä»BERTçš„è¾“å‡ºä¸­æå–æœ‰ç”¨çš„éƒ¨åˆ†ï¼Œå˜æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„æ•°å­—å‘é‡
    # ç®€å•è¯´å°±æ˜¯æŠŠä¸€å¥è¯çš„æ‰€æœ‰è¯çš„å‘é‡å–ä¸ªå¹³å‡å€¼
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()
    return embedding.astype(np.float32)  # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´



# 3. äº”æŠ˜äº¤å‰éªŒè¯ (setting up the validation procedure)
#    ï¼ˆä¸€ç§æ›´å¯é çš„æµ‹è¯•æ–¹æ³•ï¼ŒæŠŠæ•°æ®åˆ†æˆ5ä»½ï¼Œè½®æµå½“æµ‹è¯•é›†ï¼‰

# åˆå§‹åŒ–5æŠ˜äº¤å‰éªŒè¯ï¼Œæ‰“ä¹±æ•°æ®é¡ºåºï¼Œå›ºå®šéšæœºæ•°ä¿è¯ç»“æœä¸€è‡´
kf = KFold(n_splits=5, shuffle=True, random_state=SEED)

# ç”¨ä¸€ä¸ªå­—å…¸æ¥å­˜æ‰€æœ‰æµ‹è¯•ç»“æœ
all_metrics = defaultdict(list)
# å­˜å‚¨æ¯ä¸€è½®çš„æœ€ä¼˜å‚æ•°
all_best_params = []
# å­˜å‚¨æ¯ä¸€è½®çš„PCAä¿¡æ¯
all_pca_info = []

# å¾ªç¯å¤„ç†æ¯ä¸€ä»½æ•°æ®ï¼ˆä¸€å…±5ä»½ï¼‰
for fold, (train_idx, test_idx) in enumerate(kf.split(users), 1):
    # å¾—åˆ°è¿™ä¸€è½®ç”¨æ¥è®­ç»ƒå’Œæµ‹è¯•çš„ç”¨æˆ·ID
    train_users = [users[i] for i in train_idx]
    test_users = [users[i] for i in test_idx]

    # æ ¹æ®ç”¨æˆ·IDï¼Œä»æ€»æ•°æ®ä¸­é€‰å‡ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_data = data[data["ID"].isin(train_users)]
    test_data = data[data["ID"].isin(test_users)]

    # æ‰“å°è¿™ä¸€è½®çš„åŸºæœ¬ä¿¡æ¯
    print(f"\n===== ç¬¬ {fold} è½® =====")
    print(f"è®­ç»ƒé›†ç”¨æˆ·æ•°: {len(train_users)}ï¼Œæµ‹è¯•é›†ç”¨æˆ·æ•°: {len(test_users)}")

    # æŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—å‘é‡ï¼ˆç”¨ä¸Šé¢å®šä¹‰çš„BERTæ–¹æ³•ï¼‰- æ‰¹é‡å¤„ç†ä¼˜åŒ–
    print(f"    ğŸ”„ æ­£åœ¨å¤„ç†è®­ç»ƒé›†æ–‡æœ¬ï¼ˆ{len(train_data)} æ¡ï¼‰...")
    
    # ä¸ºCPUæ¨¡å¼å¯ç”¨æ‰¹é‡å¤„ç†ä¼˜åŒ–
    if DEVICE.type == 'cpu':
        # CPUæ‰¹é‡å¤„ç†ï¼Œå‡å°‘Pythonå¾ªç¯å¼€é”€
        import concurrent.futures
        from functools import partial
        
        def process_texts_batch(texts, batch_size=32):
            """æ‰¹é‡å¤„ç†æ–‡æœ¬ï¼Œæé«˜CPUæ•ˆç‡"""
            embeddings = []
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_embeddings = [get_bert_embedding(t) for t in batch_texts]
                embeddings.extend(batch_embeddings)
                if i % (batch_size * 10) == 0:
                    print(f"      å¤„ç†è¿›åº¦: {min(i+batch_size, len(texts))}/{len(texts)}")
            return embeddings
        
        train_embeddings = np.array(process_texts_batch(train_data["text"].tolist()))
        print(f"    ğŸ”„ æ­£åœ¨å¤„ç†æµ‹è¯•é›†æ–‡æœ¬ï¼ˆ{len(test_data)} æ¡ï¼‰...")
        test_embeddings = np.array(process_texts_batch(test_data["text"].tolist()))
    else:
        # GPU/MPSæ¨¡å¼ï¼šæ­£å¸¸å¤„ç†
        train_embeddings = np.array([get_bert_embedding(t) for t in train_data["text"]])
        print(f"    ğŸ”„ æ­£åœ¨å¤„ç†æµ‹è¯•é›†æ–‡æœ¬ï¼ˆ{len(test_data)} æ¡ï¼‰...")
        test_embeddings = np.array([get_bert_embedding(t) for t in test_data["text"]])

    # å†…å­˜æ¸…ç†ï¼ˆæ ¹æ®è®¾å¤‡ç±»å‹ï¼‰
    if DEVICE.type == 'cuda':
        torch.cuda.empty_cache()
    elif DEVICE.type == 'mps':
        torch.mps.empty_cache()  # Apple Siliconå†…å­˜æ¸…ç†

    print(f"    ğŸ“Š BERTåµŒå…¥å®Œæˆ: è®­ç»ƒé›† {train_embeddings.shape}, æµ‹è¯•é›† {test_embeddings.shape}")

    # ===== PCAç‰¹å¾é€‰æ‹©å’Œé™ç»´ =====
    print(f"    ğŸ”§ æ­£åœ¨è¿›è¡ŒPCAç‰¹å¾é€‰æ‹©å’Œæ ‡å‡†åŒ–...")
    
    # 1. æ ‡å‡†åŒ–ï¼ˆPCAä¹‹å‰å¿…é¡»æ ‡å‡†åŒ–ï¼‰
    scaler = StandardScaler()
    train_embeddings_scaled = scaler.fit_transform(train_embeddings)
    test_embeddings_scaled = scaler.transform(test_embeddings)
    
    # 2. PCAé™ç»´
    pca = PCA(n_components=PCA_COMPONENTS, random_state=SEED)
    train_embeddings_pca = pca.fit_transform(train_embeddings_scaled)
    test_embeddings_pca = pca.transform(test_embeddings_scaled)
    
    # è®¡ç®—è§£é‡Šæ–¹å·®æ¯”ä¾‹
    explained_variance = np.sum(pca.explained_variance_ratio_)
    print(f"    ğŸ“ˆ PCAé™ç»´å®Œæˆ: {train_embeddings.shape[1]} â†’ {PCA_COMPONENTS} ç»´")
    print(f"    ğŸ“ˆ ä¿ç•™ä¿¡æ¯é‡: {explained_variance:.4f} ({explained_variance*100:.2f}%)")
    
    # ä½¿ç”¨PCAå¤„ç†åçš„ç‰¹å¾
    train_embeddings = train_embeddings_pca
    test_embeddings = test_embeddings_pca

    # æå–è®­ç»ƒå’Œæµ‹è¯•çš„æ ‡ç­¾ï¼ˆæˆ‘ä»¬è¦é¢„æµ‹çš„ç»“æœï¼‰
    train_Labels = train_data["Label"].values
    test_Labels = test_data["Label"].values

    # 4. ç”¨é€»è¾‘å›å½’åšåˆ†ç±» + ç½‘æ ¼æœç´¢ä¼˜åŒ–è¶…å‚æ•°
    # (Logistic Regression with Grid Search for hyperparameter optimization)

    # å®šä¹‰ç½‘æ ¼æœç´¢çš„å‚æ•°ç©ºé—´
    # Define the parameter space for grid search
    param_grid = {
        'C': [1, 10],                              # æ­£åˆ™åŒ–å‚æ•°ï¼šæœ€å¸¸ç”¨çš„å€¼
        'penalty': ['l2'],                         # æ­£åˆ™åŒ–ç±»å‹ï¼šL2æœ€ç¨³å®šä¸”å¿«é€Ÿ
        'solver': ['lbfgs'],                       # ä¼˜åŒ–ç®—æ³•ï¼šé»˜è®¤ä¸”é«˜æ•ˆçš„æ±‚è§£å™¨
        'max_iter': [1000]                        # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼šé€šå¸¸è¶³å¤Ÿæ”¶æ•›
    }
    
    print(f"    å¼€å§‹ç½‘æ ¼æœç´¢æœ€ä¼˜é€»è¾‘å›å½’å‚æ•°ï¼ˆæ ·æœ¬æ•°: {len(train_embeddings)}ï¼‰...")
    print(f"    æœç´¢ç©ºé—´: C={param_grid['C']}, penalty={param_grid['penalty']}, solver={param_grid['solver']}")
    
    # åˆå§‹åŒ–ç½‘æ ¼æœç´¢
    # Initialize grid search with 3-fold cross-validation within training data
    grid_search = GridSearchCV(
        LogisticRegression(random_state=SEED),      # åŸºç¡€é€»è¾‘å›å½’æ¨¡å‹
        param_grid,                                 # å‚æ•°æœç´¢ç©ºé—´
        cv=2,                                      # 2æŠ˜äº¤å‰éªŒè¯ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
        scoring='f1_macro',                        # ä¼˜åŒ–F1åˆ†æ•°ï¼ˆé€‚åˆä¸å¹³è¡¡æ•°æ®ï¼‰
        n_jobs=-1,                                 # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒåŠ é€Ÿ
        verbose=1                                  # æ˜¾ç¤ºè¿›åº¦ä¾¿äºç›‘æ§
    )
    
    # è®­ç»ƒæ¨¡å‹å¹¶æœç´¢æœ€ä¼˜å‚æ•°
    print(f"    æ­£åœ¨è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹å¹¶æœç´¢æœ€ä¼˜è¶…å‚æ•°...")
    grid_search.fit(train_embeddings, train_Labels)
    
    # è·å–æœ€ä¼˜æ¨¡å‹
    clf = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    
    print(f"    æœ€ä¼˜å‚æ•°: {best_params}")
    print(f"    äº¤å‰éªŒè¯æœ€ä¼˜F1åˆ†æ•°: {best_score:.4f}")

    # Use the trained model to make predictions on the test set
    print(f"    Predicting on test set (number of samples: {len(test_embeddings)})...")
    pred_Labels = clf.predict(test_embeddings)  # Predicted class labels (e.g., yes/no)
    pred_probs = clf.predict_proba(test_embeddings)[:, 1]  # Predicted probability for the positive class
    
    # 5. è¯„ä¼°æ¨¡å‹å¥½å

    # è®¡ç®—å„ç§è¯„ä»·æŒ‡æ ‡
    acc = accuracy_score(test_Labels, pred_Labels)  # å‡†ç¡®ç‡ï¼šé¢„æµ‹å¯¹çš„æ¯”ä¾‹
    # ç²¾ç¡®ç‡ï¼šé¢„æµ‹ä¸ºæ­£çš„é‡Œé¢ï¼ŒçœŸæ­£ä¸ºæ­£çš„æ¯”ä¾‹
    precision = precision_score(test_Labels, pred_Labels, average="macro", zero_division=0)
    recall = recall_score(test_Labels, pred_Labels, average="macro", zero_division=0)  # å¬å›ç‡ï¼šçœŸæ­£ä¸ºæ­£çš„é‡Œé¢ï¼Œé¢„æµ‹å¯¹çš„æ¯”ä¾‹
    f1 = f1_score(test_Labels, pred_Labels, average="macro", zero_division=0)  # F1å€¼ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„ç»¼åˆæŒ‡æ ‡
    auc = roc_auc_score(test_Labels, pred_probs)  # AUCå€¼ï¼šè¡¡é‡æ¨¡å‹åŒºåˆ†æ­£è´Ÿç±»çš„èƒ½åŠ›

    # è®¡ç®—æ··æ·†çŸ©é˜µçš„å››ä¸ªåŸºæœ¬å€¼
    # tn: å®é™…æ˜¯è´Ÿï¼Œé¢„æµ‹ä¹Ÿæ˜¯è´Ÿï¼›fp: å®é™…æ˜¯è´Ÿï¼Œé¢„æµ‹æ˜¯æ­£
    # fn: å®é™…æ˜¯æ­£ï¼Œé¢„æµ‹æ˜¯è´Ÿï¼›tp: å®é™…æ˜¯æ­£ï¼Œé¢„æµ‹æ˜¯æ­£
    tn, fp, fn, tp = confusion_matrix(test_Labels, pred_Labels).ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # ç‰¹å¼‚æ€§ï¼šå®é™…ä¸ºè´Ÿçš„é‡Œé¢ï¼Œé¢„æµ‹å¯¹çš„æ¯”ä¾‹
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # é˜´æ€§é¢„æµ‹å€¼ï¼šé¢„æµ‹ä¸ºè´Ÿçš„é‡Œé¢ï¼Œå®é™…ä¸ºè´Ÿçš„æ¯”ä¾‹

    # æ‰“å°è¿™ä¸€è½®çš„è¯„ä»·ç»“æœ
    print(f"ç¬¬ {fold} è½®ç»“æœ | å‡†ç¡®ç‡: {acc:.4f}, ç²¾ç¡®ç‡: {precision:.4f}, å¬å›ç‡: {recall:.4f}, "
          f"F1å€¼: {f1:.4f}, AUCå€¼: {auc:.4f}, ç‰¹å¼‚æ€§: {specificity:.4f}, é˜´æ€§é¢„æµ‹å€¼: {npv:.4f}")

    # æŠŠè¿™ä¸€è½®çš„ç»“æœå­˜èµ·æ¥ï¼Œåé¢ç®—å¹³å‡å€¼
    all_metrics["å‡†ç¡®ç‡"].append(acc)
    all_metrics["ç²¾ç¡®ç‡"].append(precision)
    all_metrics["å¬å›ç‡"].append(recall)
    all_metrics["F1å€¼"].append(f1)
    all_metrics["AUCå€¼"].append(auc)
    all_metrics["ç‰¹å¼‚æ€§"].append(specificity)
    all_metrics["é˜´æ€§é¢„æµ‹å€¼"].append(npv)
    all_best_params.append(best_params)
    all_pca_info.append({
        'explained_variance': explained_variance,
        'components_shape': pca.components_.shape,
        'n_components': PCA_COMPONENTS
    })


# 6. æ±‡æ€»æ‰€æœ‰è½®çš„ç»“æœ

print("\n===== æœ€ç»ˆç»“æœ (5è½®çš„å¹³å‡å€¼ Â± æ³¢åŠ¨èŒƒå›´) =====")
for k, v in all_metrics.items():
    print(f"{k}: {np.mean(v):.4f} Â± {np.std(v):.4f}")

print("\n===== ç½‘æ ¼æœç´¢ç»“æœæ±‡æ€» =====")
print("å„è½®æœ€ä¼˜å‚æ•°:")
for i, params in enumerate(all_best_params, 1):
    print(f"ç¬¬ {i} è½®: {params}")

# ç»Ÿè®¡æœ€å¸¸ç”¨çš„å‚æ•°ç»„åˆ
from collections import Counter
print("\nå‚æ•°ä½¿ç”¨é¢‘ç‡ç»Ÿè®¡:")

# ç»Ÿè®¡å„å‚æ•°çš„ä½¿ç”¨é¢‘ç‡
c_values = [params['C'] for params in all_best_params]
penalty_values = [params['penalty'] for params in all_best_params]
solver_values = [params['solver'] for params in all_best_params]

print(f"Cå€¼é¢‘ç‡: {dict(Counter(c_values))}")
print(f"penaltyé¢‘ç‡: {dict(Counter(penalty_values))}")
print(f"solveré¢‘ç‡: {dict(Counter(solver_values))}")

# æ‰¾å‡ºæœ€å¸¸ç”¨çš„å‚æ•°ç»„åˆ
param_combinations = [str(params) for params in all_best_params]
most_common = Counter(param_combinations).most_common(1)[0]
print(f"\næœ€å¸¸ç”¨å‚æ•°ç»„åˆ: {most_common[0]} (å‡ºç° {most_common[1]} æ¬¡)")

print("\n===== ç½‘æ ¼æœç´¢ä¼˜åŒ–å»ºè®® =====")
print("åŸºäºä»¥ä¸Šç»“æœï¼Œå»ºè®®çš„é€»è¾‘å›å½’å‚æ•°é…ç½®:")
c_mode = Counter(c_values).most_common(1)[0][0]
penalty_mode = Counter(penalty_values).most_common(1)[0][0]
solver_mode = Counter(solver_values).most_common(1)[0][0]
print(f"æ¨èé…ç½®: C={c_mode}, penalty='{penalty_mode}', solver='{solver_mode}'")

print("\n===== PCAç‰¹å¾é€‰æ‹©åˆ†æ =====")
avg_explained_variance = np.mean([info['explained_variance'] for info in all_pca_info])
print(f"å¹³å‡ä¿¡æ¯ä¿ç•™ç‡: {avg_explained_variance:.4f} ({avg_explained_variance*100:.2f}%)")
print(f"ç‰¹å¾ç»´åº¦: {768} â†’ {PCA_COMPONENTS} (é™ç»´ {((768-PCA_COMPONENTS)/768)*100:.1f}%)")

# è®¾å¤‡ä½¿ç”¨æƒ…å†µæ±‡æ€»
if DEVICE.type == 'cuda':
    print(f"\nğŸš€ NVIDIA GPUåŠ é€Ÿæ•ˆæœ:")
    print(f"   è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"   å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**2:.1f} MB")
    print(f"   æœ€å¤§å†…å­˜: {torch.cuda.max_memory_allocated()/1024**2:.1f} MB")
    acceleration_factor = 3
elif DEVICE.type == 'mps':
    print(f"\nğŸ Apple Silicon MPSåŠ é€Ÿæ•ˆæœ:")
    print(f"   ç»Ÿä¸€å†…å­˜æ¶æ„: é«˜æ•ˆå†…å­˜è®¿é—®")
    print(f"   ç¥ç»å¼•æ“: AIä»»åŠ¡åŠ é€Ÿ")
    print(f"   åŠŸè€—ä¼˜åŒ–: æ€§èƒ½/åŠŸè€—æ¯”ä¼˜ç§€")
    acceleration_factor = 2
else:
    print(f"\nğŸ’» CPUä¼˜åŒ–æ•ˆæœ:")
    print(f"   å¤šçº¿ç¨‹å¤„ç†: {os.cpu_count()} çº¿ç¨‹")
    print(f"   æ‰¹é‡å¤„ç†: å‡å°‘Pythonå¼€é”€")
    print(f"   å†…å­˜ä¼˜åŒ–: é«˜æ•ˆç¼“å­˜ä½¿ç”¨")
    acceleration_factor = 1

print(f"\nğŸ¯ æ€§èƒ½ä¼˜åŒ–æ€»ç»“:")
device_status = "âœ…" if DEVICE.type != 'cpu' else "âš¡"
print(f"   BERTç‰¹å¾æå–: {device_status} {device_name}")
print(f"   PCAé™ç»´ä¼˜åŒ–: âœ… ({768} â†’ {PCA_COMPONENTS} ç»´)")
print(f"   ç½‘æ ¼æœç´¢ä¼˜åŒ–: âœ… (æœ€ä¼˜è¶…å‚æ•°è‡ªåŠ¨é€‰æ‹©)")
print(f"   é¢„æœŸè®­ç»ƒåŠ é€Ÿ: ~{acceleration_factor}x (è®¾å¤‡) + ~{768/PCA_COMPONENTS:.1f}x (PCAé™ç»´)")

if DEVICE.type == 'mps':
    print(f"   Apple Silicon: å·²æ£€æµ‹å¹¶å¯ç”¨ä¼˜åŒ– ğŸš€")
else:
    print(f"   å»ºè®®: å‡çº§åˆ°Apple Silicon MacBook Proè·å¾—æœ€ä½³æ€§èƒ½")
